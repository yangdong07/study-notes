

# Big Data Essentials


# Big Data Essentials: HDFS, MapReduce and Spark RDD


## HDFS: Hadoop Distributed File System

å‚è€ƒèµ„æ–™ ï¼š

- <https://wiki.apache.org/hadoop/>
- <https://www.ibm.com/developerworks/cn/web/wa-introhdfs/index.html>

Hadoopåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿã€‚

> Apache Hadoop is a framework for running applications on large cluster built of commodity hardware.


ä¸€äº›æ ¸å¿ƒæ€æƒ³ï¼š

1. å°†å¤„ç†é€»è¾‘æ”¾ç½®åˆ°æ•°æ®é™„è¿‘é€šå¸¸æ¯”å°†æ•°æ®ç§»å‘åº”ç”¨ç¨‹åºç©ºé—´æ›´å¥½ã€‚ é€šä¿—è¯´ï¼Œå°±æ˜¯å°†è¿ç®—æ”¾åˆ°æ•°æ®é™„è¿‘ã€‚



### DataNode å’Œ NameNode

å‚è€ƒ <http://itm-vm.shidler.hawaii.edu/HDFS/ArchDocOverview.html>

HDFS åŒ…æ‹¬ä¸¤ç§èŠ‚ç‚¹å½¢å¼ï¼š NameNodeå’ŒDataNodeã€‚

NameNodeç®¡ç†æ‰€æœ‰çš„DataNodeï¼Œå­˜å‚¨æ–‡ä»¶ï¼ˆblocksï¼‰çš„metadataã€‚ ä¿å­˜åœ¨RAMä¸­ï¼Œé€Ÿåº¦å¿«ã€‚

DataNodeæ˜¯å®é™…å­˜å‚¨æ–‡ä»¶çš„èŠ‚ç‚¹ã€‚ å…¶å­˜å‚¨å½¢å¼æ˜¯ blocksã€‚

å±‚çº§ç»“æ„ï¼š Cluster -> Rackï¼ˆæœºæŸœï¼‰-> PCï¼ˆä¸»æœºï¼‰-> DataNode -> blocks


#### NameNode  RAMå¤§å° çš„ä¼°ç®—æ–¹å¼

> Estimate minimum Namenode RAM size for HDFS with 1 PB capacity, block size 64 MB, average metadata size for each block is 300 B, replication factor is 3. Provide the formula for calculations and the result.

è®¡ç®—æ–¹å¼ï¼š  1PBï¼ˆ1PB = 1024TB = 1024x1024GB) å®¹é‡ï¼Œ replicationå› å­ä¸º 3ï¼Œæ¯ä¸ªblockå¤§å°ä¸º 64MBï¼Œåˆ™ æœ€å¤šåŒ…å«ï¼š1 PB / (64 MB * 3)  ä¸ªblockçš„metadataä¿¡æ¯ã€‚ä¸ºä»€ä¹ˆè¿™æ ·è®¡ç®—ï¼Ÿå› ä¸ºæ¯ä¸ªblockæœ‰3ä»½ï¼Œä½†åªéœ€è¦ä¸€ä»½metadataã€‚æ‰€ä»¥æ€»å…±éœ€è¦çš„Namenodeçš„RAMç©ºé—´ä¸ºï¼š 1 PB / (64MB x 3) x 300B = 1024 * 1024 * 1024 / 64 / 3 * 300B = 1.6GB

å‚è€ƒ <https://www.cloudera.com/documentation/enterprise/5-10-x/topics/admin_nn_memory_config.html> çš„è®¡ç®—æ–¹å¼æœ‰ç‚¹åŒºåˆ«ã€‚


#### NameNode seek timeä»¥åŠ blockè¯»å–æ—¶é—´

è¯»å–ä¸€ä¸ªblockåŒ…æ‹¬ä¸¤æ­¥ï¼š 1. åœ¨namenodeé‡Œseek blockçš„ä½ç½®ï¼› 2. è¯»å–blockã€‚

ä¸€èˆ¬è¦æ±‚seek blockæ—¶é—´æ˜¯è¯»å–ä¸€ä¸ªblockæ—¶é—´çš„ 1%ã€‚å¯ä»¥é€šè¿‡è¿™ä¸ªæ ‡å‡†ï¼ˆæˆ–è¦æ±‚ï¼‰æ¥å†³å®šblockçš„å¤§å°ã€‚

ä¾‹å¦‚ï¼š SamSung940 PRO SSD:
- reading speed  : 3.5GB/sec
- Namenode RAM seek time:  0.2~0.8ms  , 1% ã€‚
- ä¼°è®¡ blockè¯»å–æ—¶é—´ ï¼š  20~80ms
- ä¼°è®¡ blockå¤§å°ï¼š  3.5GB/sec * 20~80ms = 70MB ~ 280MB ä¹‹é—´ã€‚
- ä½¿ç”¨ blockå¤§å°ï¼š128MB
- å®é™… 128MB è¯»å–æ—¶é—´ï¼š 30~40msã€‚


## HDFS å‘½ä»¤

HDFS çš„æ–‡ä»¶ç³»ç»Ÿæ“ä½œå‘½ä»¤ï¼Œç±»ä¼¼äºè¿™ç§  `hdfs dfs -<uility>`

ä¾‹å¦‚ï¼š
- `hdfs dfs -ls`
- `hdfs dfs -mkdir -p`
- `hdfs dfs -chmod`
- `hdfs dfs -find`

å¤§éƒ¨åˆ†ä¸unixæ–‡ä»¶ç³»ç»Ÿå‘½ä»¤ç±»ä¼¼ã€‚ä¸æ¸…æ¥šçš„æŸ¥ä¸€ä¸‹helpå³å¯ã€‚è¿™é‡Œä»‹ç»ä¸¤ä¸ªç‰¹æ®Šå‘½ä»¤ï¼š

1. `hdfs dfs -setrep -w 1 xxx.txt`
è¿™ä¸ªæ˜¯è®¾ç½® replication levelã€‚
2. `hdfs fsck /user/xxx  -files -blocks -locations`
fsckæ˜¯HDFS filesystem checking utility ï¼Œ ç”¨äºåˆ—å‡ºæ–‡ä»¶çš„blockä½ç½®ï¼ŒçŠ¶æ€ç­‰ä¿¡æ¯ï¼ˆåº”è¯¥æ˜¯è®¿é—®NamenodeèŠ‚ç‚¹è·å–ï¼‰
3. `hdfs fsck -blockId blk_1032323232`
æŒ‰ç…§ blockIdè·å–blockçš„ä¿¡æ¯ã€‚ æ³¨æ„è¿™é‡Œ blockId ä¸è¦åŠ ä¸Š æ—¶é—´æˆ³ä¿¡æ¯ï¼ˆåˆ—å‡ºçš„blockIdçš„æ ¼å¼æ˜¯ `blk_<id>_<timestamp>`ï¼Œæ³¨æ„å³å¯ã€‚

å…³äº fsckçš„ usageï¼š

```
Usage: hdfs fsck <path> [-list-corruptfileblocks | [-move | -delete | -openforwrite] [-files [-blocks [-locations | -racks]]]] [-includeSnapshots] [-storagepolicies] [-blockId <blk_
Id>]
        <path>  start checking from this path
        -move   move corrupted files to /lost+found
        -delete delete corrupted files
        -files  print out files being checked
        -openforwrite   print out files opened for write
        -includeSnapshots       include snapshot data if the given path indicates a snapshottable directory or there are snapshottable directories under it
        -list-corruptfileblocks print out list of missing blocks and files they belong to
        -blocks print out block report
        -locations      print out locations for every block
        -racks  print out network topology for data-node locations
        -storagepolicies        print out storage policy summary for the blocks
        -blockId        print out which file this blockId belongs to, locations (nodes, racks) of this block, and other diagnostics info (under replicated, corrupted or not, etc)
```

## Data Modeling and File Formats

æ¯ä¸ªå¤§æ•°æ®è¯¾ç¨‹éƒ½ä¼šæåˆ°çš„ä¸¤ä¸ªæ¦‚å¿µã€‚

1. Data Modeling æ˜¯å¤„ç†æ•°æ®æ—¶ï¼ŒåŒ…æ‹¬åœ¨ç¨‹åºä¸­ä½¿ç”¨ä¸­ç”¨çš„æ¦‚å¿µï¼š ä»¥ä½•ç§æ–¹å¼ç»„ç»‡æ•°æ®ï¼Œæœ‰æ•ˆçš„æ“ä½œæ•°æ®ã€‚
2. File Formats æ˜¯å­˜å‚¨æ•°æ®æ—¶ï¼Œç”¨åˆ°çš„æ¦‚å¿µï¼š å¦‚ä½•æœ‰æ•ˆçš„å­˜å‚¨æ•°æ®ã€ä¼ è¾“æ•°æ®ï¼Œç¡®ä¿æ•°æ®çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§ï¼Ÿ

### Relational data model

å…³ç³»æ€§æ•°æ®æ¨¡å‹ã€‚ Tableã€Rowã€Columnã€‚

### Graph data model

ç‚¹ï¼ˆVerticesï¼Œentitiesï¼‰ã€è¾¹ï¼ˆEdgesï¼Œrelationsï¼‰ã€‚

### Unstructured Data

Technically, all data is structured at least as a byte sequence. Usually, means "not structured enough for a task"

å°±æ˜¯å¯¹ä¸€ä¸ªä»»åŠ¡æ¥è¯´ï¼Œç»“æ„åŒ–ä¸å¤Ÿçš„æ„æ€ã€‚

### File Format(Storage Format)

Primary Function: to transform between raw bytes and programmatical data structures(serialization & deserialization)

æœ€å…³é”®çš„åŠŸèƒ½æ˜¯åœ¨å­˜å‚¨æ ¼å¼ï¼ˆraw bytesï¼‰ ä¸ ç¨‹åºæ•°æ®æ ¼å¼ä¹‹é—´è½¬æ¢ã€‚

ä¸€èˆ¬æœ‰å‡ ä¸ªæŒ‡æ ‡è¯„ä¼°ä¸€ä¸ª File Format

|File Format| Space Efficiency | (Encoding /Decoding) Speed | Data Types | Splittable| Extensibility
|--|--|--|--|--|--|
| CSV & TSV| BAD å› ä¸ºéƒ½æ˜¯å­—ç¬¦ä¸²ï¼Œæ•°å­—ä¹Ÿç”¨å­—ç¬¦ä¸²| GOOD| Only Strings| Splittable W/O Header| BAD
| JSON | BAD(worse than CSV) | GOOD ENOUGH| Strings, numbers, booleans, maps, lists| Splittable if 1 document per line | YES


|Binary Format| Space Efficiency | (Encoding /Decoding) Speed | Data Types | Splittable| Extensibility
|--|--|--|--|--|--|
| SequenceFile| Moderate to GOOD | GOOD | ANY W/SER./DESER. CODE| Splittable | NO
| Avro| Moderate to GOOD| GOOD With CodeGen | JSON-LIKE | Splittable | YES
| RCFile|GOOD| Moderate to GOOD, Less I/O | Byte Strings | Splittable | NO
| Parquet|

æ³¨ï¼š 
1. Sequence Fileæ˜¯  First binary format implemented in Hadoopã€‚ å­˜å‚¨ é”®å€¼å¯¹åºåˆ—ã€‚æ¯ä¸ªrecordåŒ…å«ä¸€ä¸ªé”®å€¼ä¿¡æ¯ã€‚æˆ–è€…æ¯ä¸ªblockåŒ…å«è‹¥å¹²recordsã€‚recordsæˆ–è€…blocksåºåˆ—å­˜å‚¨ã€‚å…·ä½“è§ <https://wiki.apache.org/hadoop/SequenceFile>


### Compression

- Block-level
- File-level


|Codecs | Compression Speed| Decompression| Ratio|
|--|--|--|--|
|Bzip2   | 12~14MiB/s  |38~42MiB/s   | 4.02~4.80  |
|Gzip| 16~90MiB/s| 250~320MiB/s| 2.77~3.43|
|LZO| 77~150MiB/s | 290~314MiB/s| 2.10~2.48
|Snappy| 200MiB/s| 475MiB/s| 2.05|

[MiB = mebibyte](https://en.wikipedia.org/wiki/Mebibyte)

ä»€ä¹ˆæ—¶å€™ç”¨ Compressionï¼Ÿ

- CPU-boundï¼š ä¸è¦ç”¨å‹ç¼©ã€‚ç”¨å‹ç¼©ä¸ä»…ä¸ä¼šæé«˜æ•ˆç‡ï¼Œåè€Œä¼šå¢åŠ CPUè®¡ç®—è´Ÿæ‹…
- I/O-boundï¼š å¯ä»¥ä»å‹ç¼©ä¸­æ”¶ç›Šã€‚


## Hadoop MapReduce

MapReduce æ¡†æ¶æ˜¯å¾ˆé‡è¦çš„ç¨‹åºæ¨¡å‹ã€‚

å‚è€ƒ

- <https://www.zhihu.com/question/23345991>



### Unreliable Components

Node Failure:

- Fail-Stopï¼ŒæŸä¸ªèŠ‚ç‚¹æŒ‚æ‰äº†
- Fail-Recoveryï¼šæŸä¸ªèŠ‚ç‚¹æŒ‚æ‰äº†ï¼Œä½†æ˜¯ä¼šè‡ªåŠ¨æ¢å¤
- Byzantineï¼š æŸä¸ªèŠ‚ç‚¹çš„è¡Œä¸ºä¸å¯é¢„çŸ¥ï¼šä»»ä½•æƒ…å†µéƒ½æœ‰å¯èƒ½ï¼Œä¿¡æ¯å¯èƒ½è¢«ç¯¡æ”¹ï¼Œç­‰ç­‰ã€‚

æ¶‰åŠåˆ° æ‹œå åº­å°†å†›é—®é¢˜ï¼š åˆ†å¸ƒå¼å¯¹ç­‰ç½‘ç»œé€šä¿¡å®¹é”™é—®é¢˜ã€‚


### MapReduce æ¦‚å¿µ åŠå®¹é”™æ€§ï¼ˆFault Toleranceï¼‰

Map -> Shuffle & Sort -> Reduce

åœ¨Mapè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹å¦‚æœå‡ºç°é”™è¯¯ï¼Œå¯ä»¥é‡æ–°è®¡ç®—ï¼Œä¸å½±å“å…¶ä»–èŠ‚ç‚¹ã€‚

åœ¨ Shuffle & Sortè¿‡ç¨‹ä¸­ï¼Œä¸»è¦äº§ç”Ÿ intermediate filesï¼š åœ¨æœ¬åœ°ç£ç›˜ï¼Œ å¦‚æœè¿™ä¸ªè¿‡ç¨‹ä¸­æœ‰ç¼ºå¤±ï¼Œå¯ä»¥ä»Mapä¸­é‡æ–°è®¡ç®—è·å–ã€‚

Reduceè¿‡ç¨‹ä¸­ï¼Œå¦‚æœèŠ‚ç‚¹å‡ºç°é”™è¯¯ï¼Œé›†ä¸­åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹è®¡ç®—ã€‚

åœ¨ YARN ä¸­ï¼Œ é€šè¿‡ Resource Managerç®¡ç†èŠ‚ç‚¹èµ„æºã€‚ é€šè¿‡ Application Masterç®¡ç†MapReduceä»»åŠ¡ï¼ˆTaskï¼‰

MapReduce Framework çš„è¦æ±‚å°±æ˜¯è¿™äº›ï¼Œä¼šå°†è¿™äº›å·¥ä½œåœ¨åº•å±‚å®ç°å¥½ã€‚


### MapReduce Streaming

æ³¨æ„ä¸€ä¸ªæ¦‚å¿µï¼š æŠŠè®¡ç®—æ”¾åˆ°æ•°æ®é™„è¿‘ã€‚

åœ¨MapReduceçš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦çš„ä»…ä»…æ˜¯ Mapper å’Œ Reducerã€‚è¿™ä¸¤ä¸ªéƒ½å¯ä»¥é€šè¿‡è„šæœ¬ï¼ˆbashæˆ–è€…pythonï¼‰å®ç°ã€‚è¦æ±‚ä¹Ÿå¾ˆç®€å•ï¼š

- Mapper
  - define input format
  - process data
  - define output format
- Reducer
  - define input format
  - aggregate sorted data by key
  - process data
  - define output format

æ³¨æ„ è„šæœ¬ä¸å·¥ä½œæµçš„æ¥å£æ˜¯  `stdin` å’Œ `stdout`ã€‚ å¦‚æœè¦æ±‡æŠ¥ä»»åŠ¡è¿›åº¦ï¼Œåˆ™é€šè¿‡ `stderr`


#### ä¸€èˆ¬ç”¨æ³•ç¤ºä¾‹

1. mapper = `wc -l` , no reducer

```bash
HADOOP_STREAMING_JAR="/path/to/hadoop-streaming.jar"

yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'wc -l'   \
         -numRedcueTasks 0 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

2. mapper = `wc -l`,  reducer = `awk '{line_count += \$1} END {print line_count}'`

```bash
HADOOP_STREAMING_JAR="/path/to/hadoop-streaming.jar"

yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'wc -l'   \
         -reducer "awk '{line_count += \$1} END {print line_count}'"
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

3. mapper = `wc -l`, reducer = 'reducer.sh'
```bash
HADOOP_STREAMING_JAR="/path/to/hadoop-streaming.jar"

yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'wc -l'   \
         -reducer './reducer.sh' \
         -file reducer.sh  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

4. pythonè„šæœ¬
```bash
yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'python mapper.py'   \
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

#### Distributed Cache é…ç½®

`-files`, `-archives`, `-libjars`


å†æ¬¡æåŠè¿™ä¸ªæ¦‚å¿µï¼š å°†è®¡ç®—æ”¾åˆ°æ•°æ®é™„è¿‘ã€‚ä¹Ÿå°±æ˜¯è¯´ åœ¨mapreduceè®¡ç®—ä»»åŠ¡ä¸­ï¼Œå°†æ–‡ä»¶ï¼ˆfilesï¼‰ã€å‹ç¼©åŒ…ï¼ˆarchivesï¼‰ã€jaråº“ï¼ˆlibjarsï¼‰å¤åˆ¶åˆ°å„ä¸ªèŠ‚ç‚¹ï¼ˆNode Managerï¼‰ï¼Œ ç”±èŠ‚ç‚¹çš„å®¹å™¨ï¼ˆå®é™…æ‰§è¡Œä»»åŠ¡çš„å•ä½ï¼‰å»è®¿é—®è¿™äº›èµ„æºï¼Œå¹¶æ‰§è¡Œã€‚


#### ç¯å¢ƒå˜é‡

æ³¨æ„ mapperã€reducerçš„æ‰§è¡Œä½ç½®åœ¨èŠ‚ç‚¹ä¸Šã€‚å…¶ç¯å¢ƒå˜é‡åŒ…æ‹¬ï¼š

- mapreduce_task_id
- mapreduce_task_partition

ä¹Ÿå¯ä»¥ä¼ é€’ç¯å¢ƒå˜é‡å‚æ•°ï¼Œä¾‹å¦‚ï¼š

```bash
yarn jar $HADOOP_STREAMING_JAR  -D word_pattern="\w+\d+" \
         -mapper 'python mapper.py'   \
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

ä¼ é€’ä¸€ä¸ª `word_pattern` å˜é‡ï¼Œå¯ä»¥åœ¨è„šæœ¬ä¸­é€šè¿‡ `os.environ['word_pattern']` è·å–ã€‚


#### task progress

ä¸ä»…å¯ä»¥å‘ä»»åŠ¡ä¼ é€’ä¸€äº›é…ç½®ï¼Œä¹Ÿå¯ä»¥åœ¨ä»»åŠ¡æ‰§è¡Œä¸­æ±‡æŠ¥ä»»åŠ¡è¿›åº¦ã€‚

```python
import sys
print("task progress, xxx", file=sys.stderr)
```

### testing æµ‹è¯•

æµ‹è¯•åˆæ˜¯ä¸€ç³»åˆ—é—®é¢˜ã€‚

1. Unit Testingï¼Œå•å…ƒæµ‹è¯•ï¼Œ åˆ©ç”¨ä¸€äº›å·¥å…·ï¼ˆä¾‹å¦‚ pytestï¼‰å¯¹è„šæœ¬åŠŸèƒ½è¿›è¡Œæµ‹è¯•ï¼Œå•å…ƒæµ‹è¯•
2. Integration Testingï¼Œé›†æˆæµ‹è¯• ï¼Ÿï¼Ÿ
3. System Testingï¼Œ å•æœºæµ‹è¯•
4. Acceptance Testingï¼Œ å¯ä»¥ç†è§£æˆç°åº¦æµ‹è¯•ï¼Œå…ˆå¯¹ä¸€å°æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼ŒåŠæ—©å‘ç°é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œæ¨å¹¿åˆ°å…¨éƒ¨ã€‚


### Combiner

è¿™ä¸ªé—®é¢˜æˆ‘è§è¿‡ã€‚

åœ¨mapçš„æ—¶å€™ï¼Œè¾“å‡º (key, value) å€¼ã€‚ æ¯”å¦‚å•è¯è®¡æ•°è¿™ä¸ªä»»åŠ¡ã€‚ åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œçš„mapä»»åŠ¡ï¼Œå¯èƒ½äº§ç”Ÿå¾ˆå¤šä¸ªé‡å¤çš„ ï¼ˆaï¼Œ1ï¼‰è¿™æ ·çš„ã€‚ ä¸ºäº†å‡å°‘åé¢çš„å·¥ä½œé‡ï¼Œå¯ä»¥å…ˆç”¨Combinerç»„åˆä¸€ä¸‹ã€‚

åœ¨wordcountè¿™ä¸ªä¾‹å­ä¸­ï¼Œ Combiner ç­‰äº reducerçš„å¤„ç†ï¼Œå³è®¡æ•°ç»Ÿè®¡ã€‚æ‰€ä»¥æœ‰ä¸‹é¢çš„å‘½ä»¤ï¼š

```bash
yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'python mapper.py'   \
         -combiner 'python reducer.py'
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

æœ‰æ—¶å€™combineréœ€è¦å•ç‹¬å†™å¤„ç†è„šæœ¬ã€‚


### Partitioner

 å‚è€ƒ <https://blog.csdn.net/zhanglh046/article/details/78557468>

ç®€å•ä¸€å¥è¯ï¼šåˆ†åŒºå°±æ˜¯æ›´å¥½çš„å°†mapä»»åŠ¡çš„ç»“æœå‡åŒ€åˆ†é…ç»™reducerã€‚

æœ‰æ—¶å€™mapçš„è¾“å‡ºç»“æœçš„keyå€¼æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œæœ‰æ—¶å€™éœ€è¦å¤šä¸ªå­—æ®µè¿›è¡Œæ¯”è¾ƒã€‚

```bash
yarn jar $HADOOP_STREAMING_JAR \
         -D stream.num.map.output.key.fields=2
         -mapper 'python mapper.py'   \
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 5 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```
 ä¸Šé¢è¿™ä¸ªå°±æ˜¯ç”¨mapè¾“å‡ºçš„å¤´2ä¸ªå­—æ®µä½œä¸ºkeyã€‚


### Comparator

è¿™ä¸ªæ¦‚å¿µå°±ç®€å•äº†ã€‚å°±æ˜¯å®šä¹‰ shuffle & sortä¸­çš„ keyæ¯”è¾ƒå™¨ã€‚

### Speculative Task

é—®é¢˜æ˜¯ï¼šæœ‰äº›èŠ‚ç‚¹ä»»åŠ¡æ‰§è¡Œçš„æ…¢ï¼Œæœ‰äº›èŠ‚ç‚¹ä»»åŠ¡æ‰§è¡Œçš„å¿«ã€‚ æ€»çš„è¿è¡Œæ—¶é—´æ€»æ˜¯è¢«æ…¢çš„ä»»åŠ¡èŠ‚ç‚¹æ‹–æ…¢ã€‚å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ

å‚è€ƒ

- <http://dongxicheng.org/mapreduce/hadoop-speculative-task/>
- <https://stackoverflow.com/questions/15164886/hadoop-speculative-task-execution>

> One problem with the Hadoop system is that by dividing the tasks across many nodes, it is possible for a few slow nodes to rate-limit the rest of the program.

> Tasks may be slow for various reasons, including hardware degradation, or software mis-configuration, but the causes may be hard to detect since the tasks still complete successfully, albeit after a longer time than expected. Hadoop doesnâ€™t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another, equivalent, task as a backup. This is termed speculative execution of tasks.

> For example if one node has a slow disk controller, then it may be reading its input at only 10% the speed of all the other nodes. So when 99 map tasks are already complete, the system is still waiting for the final map task to check in, which takes much longer than all the other nodes.

> By forcing tasks to run in isolation from one another, individual tasks do not know where their inputs come from. Tasks trust the Hadoop platform to just deliver the appropriate input. Therefore, the same input can be processed multiple times in parallel, to exploit differences in machine capabilities. As most of the tasks in a job are coming to a close, the Hadoop platform will schedule redundant copies of the remaining tasks across several nodes which do not have other work to perform. This process is known as speculative execution. When tasks complete, they announce this fact to the JobTracker. Whichever copy of a task finishes first becomes the definitive copy. If other copies were executing speculatively, Hadoop tells the TaskTrackers to abandon the tasks and discard their outputs. The Reducers then receive their inputs from whichever Mapper completed successfully, first.

å‰é¢ä¸‰æ®µæè¿°äº†é—®é¢˜ã€‚ æœ€åä¸€æ®µè¯´æ˜ Speculative æ–¹æ¡ˆï¼š å½“ä¸€ä¸ªèŠ‚ç‚¹çš„ä»»åŠ¡è½åäºå…¶ä»–èŠ‚ç‚¹æ—¶ï¼Œåœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šï¼ˆæ²¡æœ‰ä»»åŠ¡çš„èŠ‚ç‚¹ä¸Šï¼‰å»æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œå–ä¸€ä¸ªæœ€å¿«çš„ç»“æœï¼Œå°†å…¶ä»–ä»»åŠ¡ discardæ‰ã€‚ è¿™æ˜¯ä¸€ç§ç”¨ç©ºé—´æ¢æ—¶é—´çš„åšæ³•ã€‚


### Compression


|Compression Format| Splittable| Comments| Hadoop CompressionCodec
|---|---|---|----|
|.deflate  .gz(gzip)| NO| Uses DEFLATE algorithm | org.apache.hadoop.io.compress.DefaultCodec GzipCodec
|.bz2(bzip)| YES | more effective than gzip, but slower compression | Bzip2Codec
|.lzo| YES | decompres way faster than gzip, compres less effiecient | com.hadoop.compression.lzo.LzopCodec
|.snappy| NO| faster than LZO for decompression | SnappyCodec


options
```
-D mapreduce.compress.map.output=true            # mapè¾“å‡ºå‹ç¼©
-D mapreduce.map.output.compression.codec=...    # Codecs
```
```
-D mapreduce.output.compress=true          # reduceræœ€ç»ˆç»“æœå‹ç¼©
-D mpareduce.output.compression.codec=...  # Codecs
```

å¯¹äºä¸€äº› cold dataï¼ˆä¸å¸¸ç”¨çš„dataï¼‰ï¼Œå¯ä»¥ç”¨ bzip2æ–¹å¼ï¼Œ ç©ºé—´æ•ˆç‡é«˜ï¼Œå‹ç¼©æ—¶é—´é•¿ã€‚
å¯¹äºç»å¸¸è®¿é—®çš„ hot dataï¼Œ å¯ä»¥ç”¨ lzopæ–¹å¼ï¼Œç©ºé—´æ•ˆç‡ä½ï¼Œä½†æ˜¯è§£å‹æ—¶é—´å¿«ã€‚
