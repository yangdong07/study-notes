
### Hadoop MapReduce

MapReduce æ¡†æ¶æ˜¯å¾ˆé‡è¦çš„ç¨‹åºæ¨¡å‹ã€‚

å‚è€ƒ

- <https://www.zhihu.com/question/23345991>



### Unreliable Components

Node Failure:

- Fail-Stopï¼ŒæŸä¸ªèŠ‚ç‚¹æŒ‚æ‰äº†
- Fail-Recoveryï¼šæŸä¸ªèŠ‚ç‚¹æŒ‚æ‰äº†ï¼Œä½†æ˜¯ä¼šè‡ªåŠ¨æ¢å¤
- Byzantineï¼š æŸä¸ªèŠ‚ç‚¹çš„è¡Œä¸ºä¸å¯é¢„çŸ¥ï¼šä»»ä½•æƒ…å†µéƒ½æœ‰å¯èƒ½ï¼Œä¿¡æ¯å¯èƒ½è¢«ç¯¡æ”¹ï¼Œç­‰ç­‰ã€‚

æ¶‰åŠåˆ° æ‹œå åº­å°†å†›é—®é¢˜ï¼š åˆ†å¸ƒå¼å¯¹ç­‰ç½‘ç»œé€šä¿¡å®¹é”™é—®é¢˜ã€‚


### MapReduce æ¦‚å¿µ åŠå®¹é”™æ€§ï¼ˆFault Toleranceï¼‰

Map -> Shuffle & Sort -> Reduce

åœ¨Mapè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹å¦‚æœå‡ºç°é”™è¯¯ï¼Œå¯ä»¥é‡æ–°è®¡ç®—ï¼Œä¸å½±å“å…¶ä»–èŠ‚ç‚¹ã€‚

åœ¨ Shuffle & Sortè¿‡ç¨‹ä¸­ï¼Œä¸»è¦äº§ç”Ÿ intermediate filesï¼š åœ¨æœ¬åœ°ç£ç›˜ï¼Œ å¦‚æœè¿™ä¸ªè¿‡ç¨‹ä¸­æœ‰ç¼ºå¤±ï¼Œå¯ä»¥ä»Mapä¸­é‡æ–°è®¡ç®—è·å–ã€‚

Reduceè¿‡ç¨‹ä¸­ï¼Œå¦‚æœèŠ‚ç‚¹å‡ºç°é”™è¯¯ï¼Œé›†ä¸­åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹è®¡ç®—ã€‚

åœ¨ YARN ä¸­ï¼Œ é€šè¿‡ Resource Managerç®¡ç†èŠ‚ç‚¹èµ„æºã€‚ é€šè¿‡ Application Masterç®¡ç†MapReduceä»»åŠ¡ï¼ˆTaskï¼‰

MapReduce Framework çš„è¦æ±‚å°±æ˜¯è¿™äº›ï¼Œä¼šå°†è¿™äº›å·¥ä½œåœ¨åº•å±‚å®ç°å¥½ã€‚


### MapReduce Streaming

æ³¨æ„ä¸€ä¸ªæ¦‚å¿µï¼š æŠŠè®¡ç®—æ”¾åˆ°æ•°æ®é™„è¿‘ã€‚

åœ¨MapReduceçš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦çš„ä»…ä»…æ˜¯ Mapper å’Œ Reducerã€‚è¿™ä¸¤ä¸ªéƒ½å¯ä»¥é€šè¿‡è„šæœ¬ï¼ˆbashæˆ–è€…pythonï¼‰å®ç°ã€‚è¦æ±‚ä¹Ÿå¾ˆç®€å•ï¼š

- Mapper
  - define input format
  - process data
  - define output format
- Reducer
  - define input format
  - aggregate sorted data by key
  - process data
  - define output format

æ³¨æ„ è„šæœ¬ä¸å·¥ä½œæµçš„æ¥å£æ˜¯  `stdin` å’Œ `stdout`ã€‚ å¦‚æœè¦æ±‡æŠ¥ä»»åŠ¡è¿›åº¦ï¼Œåˆ™é€šè¿‡ `stderr`


#### ä¸€èˆ¬ç”¨æ³•ç¤ºä¾‹

1. mapper = `wc -l` , no reducer

```bash
HADOOP_STREAMING_JAR="/path/to/hadoop-streaming.jar"

yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'wc -l'   \
         -numRedcueTasks 0 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

2. mapper = `wc -l`,  reducer = `awk '{line_count += \$1} END {print line_count}'`

```bash
HADOOP_STREAMING_JAR="/path/to/hadoop-streaming.jar"

yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'wc -l'   \
         -reducer "awk '{line_count += \$1} END {print line_count}'"
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

3. mapper = `wc -l`, reducer = 'reducer.sh'
```bash
HADOOP_STREAMING_JAR="/path/to/hadoop-streaming.jar"

yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'wc -l'   \
         -reducer './reducer.sh' \
         -file reducer.sh  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

4. pythonè„šæœ¬
```bash
yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'python mapper.py'   \
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

#### Distributed Cache é…ç½®

`-files`, `-archives`, `-libjars`


å†æ¬¡æåŠè¿™ä¸ªæ¦‚å¿µï¼š å°†è®¡ç®—æ”¾åˆ°æ•°æ®é™„è¿‘ã€‚ä¹Ÿå°±æ˜¯è¯´ åœ¨mapreduceè®¡ç®—ä»»åŠ¡ä¸­ï¼Œå°†æ–‡ä»¶ï¼ˆfilesï¼‰ã€å‹ç¼©åŒ…ï¼ˆarchivesï¼‰ã€jaråº“ï¼ˆlibjarsï¼‰å¤åˆ¶åˆ°å„ä¸ªèŠ‚ç‚¹ï¼ˆNode Managerï¼‰ï¼Œ ç”±èŠ‚ç‚¹çš„å®¹å™¨ï¼ˆå®é™…æ‰§è¡Œä»»åŠ¡çš„å•ä½ï¼‰å»è®¿é—®è¿™äº›èµ„æºï¼Œå¹¶æ‰§è¡Œã€‚


#### ç¯å¢ƒå˜é‡

æ³¨æ„ mapperã€reducerçš„æ‰§è¡Œä½ç½®åœ¨èŠ‚ç‚¹ä¸Šã€‚å…¶ç¯å¢ƒå˜é‡åŒ…æ‹¬ï¼š

- mapreduce_task_id
- mapreduce_task_partition

ä¹Ÿå¯ä»¥ä¼ é€’ç¯å¢ƒå˜é‡å‚æ•°ï¼Œä¾‹å¦‚ï¼š

```bash
yarn jar $HADOOP_STREAMING_JAR  -D word_pattern="\w+\d+" \
         -mapper 'python mapper.py'   \
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

ä¼ é€’ä¸€ä¸ª `word_pattern` å˜é‡ï¼Œå¯ä»¥åœ¨è„šæœ¬ä¸­é€šè¿‡ `os.environ['word_pattern']` è·å–ã€‚


#### task progress

ä¸ä»…å¯ä»¥å‘ä»»åŠ¡ä¼ é€’ä¸€äº›é…ç½®ï¼Œä¹Ÿå¯ä»¥åœ¨ä»»åŠ¡æ‰§è¡Œä¸­æ±‡æŠ¥ä»»åŠ¡è¿›åº¦ã€‚

```python
import sys
print("task progress, xxx", file=sys.stderr)
```

### testing æµ‹è¯•

æµ‹è¯•åˆæ˜¯ä¸€ç³»åˆ—é—®é¢˜ã€‚

1. Unit Testingï¼Œå•å…ƒæµ‹è¯•ï¼Œ åˆ©ç”¨ä¸€äº›å·¥å…·ï¼ˆä¾‹å¦‚ pytestï¼‰å¯¹è„šæœ¬åŠŸèƒ½è¿›è¡Œæµ‹è¯•ï¼Œå•å…ƒæµ‹è¯•
2. Integration Testingï¼Œé›†æˆæµ‹è¯• ï¼Ÿï¼Ÿ
3. System Testingï¼Œ å•æœºæµ‹è¯•
4. Acceptance Testingï¼Œ å¯ä»¥ç†è§£æˆç°åº¦æµ‹è¯•ï¼Œå…ˆå¯¹ä¸€å°æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼ŒåŠæ—©å‘ç°é—®é¢˜ã€‚å¦‚æœæ²¡æœ‰é—®é¢˜ï¼Œæ¨å¹¿åˆ°å…¨éƒ¨ã€‚


### Combiner

è¿™ä¸ªé—®é¢˜æˆ‘è§è¿‡ã€‚

åœ¨mapçš„æ—¶å€™ï¼Œè¾“å‡º (key, value) å€¼ã€‚ æ¯”å¦‚å•è¯è®¡æ•°è¿™ä¸ªä»»åŠ¡ã€‚ åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œçš„mapä»»åŠ¡ï¼Œå¯èƒ½äº§ç”Ÿå¾ˆå¤šä¸ªé‡å¤çš„ ï¼ˆaï¼Œ1ï¼‰è¿™æ ·çš„ã€‚ ä¸ºäº†å‡å°‘åé¢çš„å·¥ä½œé‡ï¼Œå¯ä»¥å…ˆç”¨Combinerç»„åˆä¸€ä¸‹ã€‚

åœ¨wordcountè¿™ä¸ªä¾‹å­ä¸­ï¼Œ Combiner ç­‰äº reducerçš„å¤„ç†ï¼Œå³è®¡æ•°ç»Ÿè®¡ã€‚æ‰€ä»¥æœ‰ä¸‹é¢çš„å‘½ä»¤ï¼š

```bash
yarn jar $HADOOP_STREAMING_JAR \
         -mapper 'python mapper.py'   \
         -combiner 'python reducer.py'
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 1 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```

æœ‰æ—¶å€™combineréœ€è¦å•ç‹¬å†™å¤„ç†è„šæœ¬ã€‚


### Partitioner

 å‚è€ƒ <https://blog.csdn.net/zhanglh046/article/details/78557468>

ç®€å•ä¸€å¥è¯ï¼šåˆ†åŒºå°±æ˜¯æ›´å¥½çš„å°†mapä»»åŠ¡çš„ç»“æœå‡åŒ€åˆ†é…ç»™reducerã€‚

æœ‰æ—¶å€™mapçš„è¾“å‡ºç»“æœçš„keyå€¼æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œæœ‰æ—¶å€™éœ€è¦å¤šä¸ªå­—æ®µè¿›è¡Œæ¯”è¾ƒã€‚

```bash
yarn jar $HADOOP_STREAMING_JAR \
         -D stream.num.map.output.key.fields=2
         -mapper 'python mapper.py'   \
         -reducer 'python reducer.py' \
         -files reducer.py,mapper.py  \
         -numRedcueTasks 5 \
         -input  /data/wiki/en_ariticls \
         -output wc_mr
```
 ä¸Šé¢è¿™ä¸ªå°±æ˜¯ç”¨mapè¾“å‡ºçš„å¤´2ä¸ªå­—æ®µä½œä¸ºkeyã€‚


### Comparator

è¿™ä¸ªæ¦‚å¿µå°±ç®€å•äº†ã€‚å°±æ˜¯å®šä¹‰ shuffle & sortä¸­çš„ keyæ¯”è¾ƒå™¨ã€‚

### Speculative Task

é—®é¢˜æ˜¯ï¼šæœ‰äº›èŠ‚ç‚¹ä»»åŠ¡æ‰§è¡Œçš„æ…¢ï¼Œæœ‰äº›èŠ‚ç‚¹ä»»åŠ¡æ‰§è¡Œçš„å¿«ã€‚ æ€»çš„è¿è¡Œæ—¶é—´æ€»æ˜¯è¢«æ…¢çš„ä»»åŠ¡èŠ‚ç‚¹æ‹–æ…¢ã€‚å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ

å‚è€ƒ

- <http://dongxicheng.org/mapreduce/hadoop-speculative-task/>
- <https://stackoverflow.com/questions/15164886/hadoop-speculative-task-execution>

> One problem with the Hadoop system is that by dividing the tasks across many nodes, it is possible for a few slow nodes to rate-limit the rest of the program.

> Tasks may be slow for various reasons, including hardware degradation, or software mis-configuration, but the causes may be hard to detect since the tasks still complete successfully, albeit after a longer time than expected. Hadoop doesnâ€™t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another, equivalent, task as a backup. This is termed speculative execution of tasks.

> For example if one node has a slow disk controller, then it may be reading its input at only 10% the speed of all the other nodes. So when 99 map tasks are already complete, the system is still waiting for the final map task to check in, which takes much longer than all the other nodes.

> By forcing tasks to run in isolation from one another, individual tasks do not know where their inputs come from. Tasks trust the Hadoop platform to just deliver the appropriate input. Therefore, the same input can be processed multiple times in parallel, to exploit differences in machine capabilities. As most of the tasks in a job are coming to a close, the Hadoop platform will schedule redundant copies of the remaining tasks across several nodes which do not have other work to perform. This process is known as speculative execution. When tasks complete, they announce this fact to the JobTracker. Whichever copy of a task finishes first becomes the definitive copy. If other copies were executing speculatively, Hadoop tells the TaskTrackers to abandon the tasks and discard their outputs. The Reducers then receive their inputs from whichever Mapper completed successfully, first.

å‰é¢ä¸‰æ®µæè¿°äº†é—®é¢˜ã€‚ æœ€åä¸€æ®µè¯´æ˜ Speculative æ–¹æ¡ˆï¼š å½“ä¸€ä¸ªèŠ‚ç‚¹çš„ä»»åŠ¡è½åäºå…¶ä»–èŠ‚ç‚¹æ—¶ï¼Œåœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šï¼ˆæ²¡æœ‰ä»»åŠ¡çš„èŠ‚ç‚¹ä¸Šï¼‰å»æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œå–ä¸€ä¸ªæœ€å¿«çš„ç»“æœï¼Œå°†å…¶ä»–ä»»åŠ¡ discardæ‰ã€‚ è¿™æ˜¯ä¸€ç§ç”¨ç©ºé—´æ¢æ—¶é—´çš„åšæ³•ã€‚


### Compression


|Compression Format| Splittable| Comments| Hadoop CompressionCodec
|---|---|---|----|
|.deflate  .gz(gzip)| NO| Uses DEFLATE algorithm | org.apache.hadoop.io.compress.DefaultCodec GzipCodec
|.bz2(bzip)| YES | more effective than gzip, but slower compression | Bzip2Codec
|.lzo| YES | decompres way faster than gzip, compres less effiecient | com.hadoop.compression.lzo.LzopCodec
|.snappy| NO| faster than LZO for decompression | SnappyCodec


options
```
-D mapreduce.compress.map.output=true            # mapè¾“å‡ºå‹ç¼©
-D mapreduce.map.output.compression.codec=...    # Codecs
```
```
-D mapreduce.output.compress=true          # reduceræœ€ç»ˆç»“æœå‹ç¼©
-D mpareduce.output.compression.codec=...  # Codecs
```

å¯¹äºä¸€äº› cold dataï¼ˆä¸å¸¸ç”¨çš„dataï¼‰ï¼Œå¯ä»¥ç”¨ bzip2æ–¹å¼ï¼Œ ç©ºé—´æ•ˆç‡é«˜ï¼Œå‹ç¼©æ—¶é—´é•¿ã€‚
å¯¹äºç»å¸¸è®¿é—®çš„ hot dataï¼Œ å¯ä»¥ç”¨ lzopæ–¹å¼ï¼Œç©ºé—´æ•ˆç‡ä½ï¼Œä½†æ˜¯è§£å‹æ—¶é—´å¿«ã€‚

### hadoop streming ç»ƒä¹ ä½œä¸š

#### wordcount

mapper.py
```python
%%writefile mapper.py

import sys
import re

reload(sys)
sys.setdefaultencoding('utf-8') # required to convert to unicode

for line in sys.stdin:
    try:
        article_id, text = unicode(line.strip()).split('\t', 1)
    except ValueError as e:
        continue
    words = re.split("\W*\s+\W*", text, flags=re.UNICODE)
    for word in words:
        print >> sys.stderr, "reporter:counter:Wiki stats,Total words,%d" % 1
        print "%s\t%d" % (word.lower(), 1)
```

reducer.py
```python
%%writefile reducer.py

import sys

current_key = None
word_sum = 0

for line in sys.stdin:
    try:
        key, count = line.strip().split('\t', 1)
        count = int(count)
    except ValueError as e:
        continue
    if current_key != key:
        if current_key:
            print "%s\t%d" % (current_key, word_sum)
        word_sum = 0
        current_key = key
    word_sum += count

if current_key:
    print "%s\t%d" % (current_key, word_sum)
```

```bash
%%bash

OUT_DIR="wordcount_result_"$(date +"%s%6N")
NUM_REDUCERS=8

hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.jab.name="Streaming wordCount" \
    -D mapreduce.job.reduces=${NUM_REDUCERS} \
    -files mapper.py,reducer.py \
    -mapper "python mapper.py" \
    -combiner "python reducer.py" \
    -reducer "python reducer.py" \
    -input /data/wiki/en_articles_part \
    -output ${OUT_DIR} > /dev/null

hdfs dfs -cat ${OUT_DIR}/part-00000 | head
```

#### æŒ‰å‡ºç°æ¬¡æ•°æœ€å¤šçš„é¡ºåºæ’åº

 è¿™é‡Œéœ€è¦ä¸¤ä¸ªä»»åŠ¡ï¼Œè¿™ä¸ªèŠ±äº†æˆ‘å¾ˆé•¿æ—¶é—´ã€‚ è¿™ä¸ªç»ƒä¹ å¤ªå‘äººäº†ã€‚é¢˜ç›®æ²¡æœ‰è¯´æ¸…æ¥šã€‚å®Œå…¨é çŒœã€‚

æ³¨æ„å‡ ç‚¹ï¼š

1. ç¬¬äºŒä¸ªä»»åŠ¡ä¸éœ€è¦è„šæœ¬ï¼Œå¯ä»¥ç›´æ¥ç”¨ `partition.keycomparator` æ’åºå®ç°ã€‚å‚è€ƒ <http://52.36.65.137/iSR_Grav/entries/entry-2#tabs1-jvp>ï¼Œä»¥åŠ sortå‘½ä»¤ä½¿ç”¨æ–¹æ³•ã€‚
2. ç¬¬äºŒä¸ªä»»åŠ¡åªç”¨ä¸€ä¸ªreducerè¾“å‡ºã€‚ç¬¬äºŒä¸ªä»»åŠ¡çš„è¾“å…¥æ˜¯ç¬¬ä¸€ä¸ªä»»åŠ¡çš„è¾“å‡ºã€‚
3. å¦‚æœè¾“å‡ºæ˜¯å€’åºçš„ï¼Œé‚£ä¹ˆç›´æ¥ç”¨ `sed`å‘½ä»¤å°±å¯ä»¥å–ç¬¬7è¡Œæ•°æ®ã€‚

```bash
%%bash

OUT_DIR="wordcount_result_"$(date +"%s%6N")
NUM_REDUCERS=8
OUT_DIR2="popular_result"

hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null
hdfs dfs -rm -r -skipTrash ${OUT_DIR2} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.jab.name="Streaming wordCount" \
    -D mapreduce.job.reduces=${NUM_REDUCERS} \
    -files mapper.py,reducer.py \
    -mapper "python mapper.py" \
    -combiner "python reducer.py" \
    -reducer "python reducer.py" \
    -input /data/wiki/en_articles_part \
    -output ${OUT_DIR} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \
    -D stream.num.map.output.key.fields=2 \
    -D stream.map.output.field.separator="\t" \
    -D mapreduce.partition.keycomparator.options="-k2,2nr" \
    -D mapred.jab.name="Streaming wordCount2" \
    -D mapreduce.job.reduces=1 \
    -files mapper.py,reducer.py \
    -mapper /bin/cat \
    -reducer /bin/cat \
    -input ${OUT_DIR} \
    -output ${OUT_DIR2} > /dev/null

hdfs dfs -cat ${OUT_DIR2}/part-00000 | sed -n "7p"
```

#### ç»Ÿè®¡ stop wordsæ•°é‡ï¼Œå¹¶è®¡ç®—å æ¯”

æ€è·¯æ˜¯åœ¨ mapperé‡ŒåŠ ä¸€æ¡ç»Ÿè®¡ä¿¡æ¯ï¼Œå½“å‡ºç°stopwordsæ—¶ï¼Œå°±è®¡1ã€‚ ç„¶ååœ¨è¾“å‡ºçš„logsé‡Œï¼Œæ‰¾åˆ°Total wordså’Œ Stop wordsä¸¤ä¸ªç»Ÿè®¡ä¿¡æ¯ï¼Œè®¡ç®—è¾“å‡ºå°±å¯ä»¥äº†ã€‚

```python
%%writefile mapper.py

...
stop_words = set()
with open("stop_words_en.txt") as f:
    for line in f:
        stop_words.add(line.strip().lower())

for line in sys.stdin:
    ...
    for word in words:
        print >> sys.stderr, "reporter:counter:Wiki stats,Total words,%d" % 1
        if word in stop_words:
            print >> sys.stderr, "reporter:counter:Wiki stats,Stop words,%d" % 1
        print "%s\t%d" % (word.lower(), 1)
```

 è§£ææ–‡ä»¶
```python
%%writefile counter_process.py

import sys

stop_words = 0
total_words = 0

with open(sys.argv[1]) as f:
    for line in f:
        line = line.strip().lower()
        if line.startswith('stop words'):
            stop_words = int(line.split('=')[1])
        elif line.startswith('total words'):
            total_words = int(line.split('=')[1])

print('%.2f' % (stop_words / float(total_words) * 100))
```

```bash
%%bash

OUT_DIR="wordcount_result"
NUM_REDUCERS=8
LOGS="stderr_logs.txt"

hdfs dfs -rm -r -skipTrash ${OUT_DIR} > /dev/null
hdfs dfs -get /datasets/stop_words_en.txt .

yarn jar ...
    -output ${OUT_DIR} > /dev/null 2> $LOGS

python ./counter_process.py $LOGS
cat $LOGS >&2
```
 æ³¨æ„ä»¥ä¸Šæ­¥éª¤ï¼Œå…ˆæ˜¯å°† stderré‡å®šå‘åˆ° `$LOGS`ï¼Œç„¶åç”¨pythonè„šæœ¬å¤„ç† `$LOGS`ï¼Œæœ€åå†å°† `$LOGS`æ”¾å‡ºæ¥ã€‚ä¸»è¦æ˜¯ä¸ºäº†è¯„åˆ†ç³»ç»Ÿå·¥ä½œã€‚


#### æ‰¾å‡ºåå­—ï¼Œç„¶åæŒ‰æ•°é‡å€’åºè¾“å‡º

è¿™ä¸€é¢˜é¢˜æ„æè¿°ç‰¹åˆ«å·®ã€‚ ä»€ä¹ˆæ˜¯åå­—ï¼Ÿ ç¬¦åˆä»¥ä¸‹æ¡ä»¶ï¼š

1. ä¸ä»¥æ•°å­—å¼€å¤´
2. ä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼Œå…¶ä»–å­—æ¯éƒ½æ˜¯å°å†™å­—æ¯
3. å¦‚æœè¿™ä¸ªå•è¯ç¬¦åˆæ¡ä»¶ï¼ˆ2ï¼‰ï¼ŒåŒæ—¶ä¹Ÿæœ‰ä¸ç¬¦åˆæ¡ä»¶ï¼ˆ2ï¼‰çš„ï¼Œåˆ™çœ‹å…¶ç»Ÿè®¡æ¯”ä¾‹ï¼Œå¦‚æœä¸ç¬¦åˆæ¡ä»¶ï¼ˆ2ï¼‰çš„å æ¯”å°äº 0.5%ï¼Œåˆ™è®¤ä¸ºæ˜¯åå­—ã€‚

æ€è·¯æ˜¯ï¼š

1. å¯¹äºç¬¦åˆæ¡ä»¶ï¼ˆ1ï¼Œ 2ï¼‰çš„ï¼ˆæŒ‰æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼‰ï¼Œmapå€¼ä¸º : (word, 1, 1)ï¼Œç¬¬ä¸€ä¸ªå€¼è¡¨ç¤º isnameï¼Œç¬¬äºŒä¸ªå€¼æ˜¯è®¡æ•°ã€‚
2. å¯¹äºå…¶ä»–æƒ…å†µï¼Œmapå€¼ä¸º: (word, 0, 1)
3. ç¬¬ä¸€ä¸ªreducerè´Ÿè´£è®¡æ•°ã€‚ ä¸ä»…ç»Ÿè®¡ countï¼Œè¿˜è¦ç»Ÿè®¡ isnameçš„ä¸ªæ•°
4. ç¬¬äºŒä¸ªmapreduce jobï¼Œ mapè´Ÿè´£è¿‡æ»¤ï¼Œ å¯¹äº isname / count > 0.995 çš„ï¼Œè¾“å‡ºï¼›å…¶ä»–éƒ½è¿‡æ»¤æ‰ã€‚
5. ç¬¬äºŒä¸ªreducerè´Ÿè´£å€’åºæ’åºï¼Œä»¥ç¬¬äºŒä¸ªå­—æ®µæ•°å€¼å€’åºæ’åºã€‚

```python
%%writefile mapper.py
#/usr/bin/python

import sys
import re

reload(sys)
sys.setdefaultencoding('utf-8') # required to convert to unicode

name_pattern = re.compile("^[A-Z][0-9a-z_]*$")

for line in sys.stdin:
    try:
        article_id, text = unicode(line.strip()).split('\t', 1)
    except ValueError as e:
        continue
    words = re.split("\W*\s+\W*", text, flags=re.UNICODE)
    for word in words:
        print >> sys.stderr, "reporter:counter:Wiki stats,Total words,%d" % 1
        if name_pattern.match(word):
            print "%s\t%d\t%d" % (word.lower(), 1, 1)
        else:
            print "%s\t%d\t%d" % (word.lower(), 0, 1)
```

```python
%%writefile reducer.py

import sys

current_key = None
word_sum = 0
isname = 0

for line in sys.stdin:
    try:
        key, match, count = line.strip().split('\t', 2)
        count = int(count)
        match = int(match)
    except ValueError as e:
#         print e
        continue
    if current_key != key:
        if current_key:
            print "%s\t%d\t%d" % (current_key, isname, word_sum)
        isname = 0
        word_sum = 0
        current_key = key
    isname += match
    word_sum += count

if current_key:
    print "%s\t%d\t%d" % (current_key, isname, word_sum)
```

```python
%%writefile mapper2.py

import sys

for line in sys.stdin:
    try:
        word, isname, count = line.strip().split('\t', 2)
        isname = int(isname)
        count = int(count)
    except ValueError:
        continue

    if isname / float(count) > 0.995:
        print "%s\t%d" % (word, count)
```

```bash
%%bash

INPUT="/data/wiki/en_articles_part"
OUTPUT1="wordcount_result"
OUTPUT2="names"

hdfs dfs -rm -r -skipTrash ${OUTPUT1} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.jab.name="Streaming wordCount" \
    -D mapreduce.job.reduces=8 \
    -files mapper.py,reducer.py \
    -mapper "python mapper.py" \
    -combiner "python reducer.py" \
    -reducer "python reducer.py" \
    -input ${INPUT} \
    -output ${OUTPUT1} > /dev/null

hdfs dfs -rm -r -skipTrash ${OUTPUT2} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \
    -D stream.num.map.output.key.fields=2 \
    -D stream.map.output.field.separator="\t" \
    -D mapreduce.partition.keycomparator.options="-k2,2nr" \
    -D mapred.jab.name="Streaming nameCount" \
    -D mapreduce.job.reduces=1 \
    -files mapper2.py \
    -mapper "python mapper2.py" \
    -reducer "/bin/cat" \
    -input ${OUTPUT1} \
    -output ${OUTPUT2} > /dev/null

! hdfs dfs -cat ${OUTPUT2}/part-00000 | sed -n '5p'

```

#### æŒ‰ç»„ç»Ÿè®¡æ•°å­—

å…¶å®å¯ä»¥åªç”¨ä¸€ä¸ªmapreducerï¼Œä½†æ˜¯é¢˜ç›®éè¦ç”¨ä¸¤ä¸ªmapreducerã€‚

1. ç¬¬ä¸€ä¸ªmapreducerè´Ÿè´£å•è¯è®¡æ•°ï¼Œè¿™ä¸ªæ²¡ä»€ä¹ˆ
2. ç¬¬äºŒä¸ªmapreducerï¼Œmapè´Ÿè´£å°† (word, count) æ˜ å°„ä¸º (key, word, count)ï¼Œkeyæ˜¯ wordæŒ‰å­—æ¯é¡ºåºæ’åˆ—åçš„å­—ç¬¦ä¸²ã€‚
3. ç¬¬äºŒä¸ªmapreducerï¼Œreducerè´Ÿè´£å°†keyå€¼ç›¸åŒçš„ï¼Œç»Ÿè®¡æ€»æ•°å’Œ unique wordè®¡æ•°ã€‚ç„¶åæ‹¼æ¥è¾“å‡ºã€‚

 è¿™é‡Œåªç»™å‡ºç¬¬äºŒä¸ªmapreducerçš„ä»£ç 

```python
%%writefile mapper2.py

import sys
import re

for line in sys.stdin:
    try:
        word, count = line.strip().split('\t', 1)
        count = int(count)
    except ValueError as e:
        continue

    key = ''.join(sorted(word))
    print "%s\t%s\t%d" % (key, word, count)
```

```python
%%writefile reducer2.py

import sys

cur_key = None
word_count = 0
words = set()

for line in sys.stdin:
    try:
        key, word, count = line.strip().split('\t', 2)
        count = int(count)
    except ValueError as e:
        continue
    if cur_key != key:
        if cur_key:
            word_list = list(words)
            word_list.sort()            
            print "%d\t%d\t%s" % (word_count, len(words), ','.join(word_list))
        word_count = 0
        words = set()        
        cur_key = key
    word_count += count
    words.add(word)

if cur_key:
    print "%d\t%d\t%s" % (word_count, len(words), ','.join(word_list))

```

```bash
%%bash

INPUT="/data/wiki/en_articles_part"
OUTPUT1="wordcount"
OUTPUT2="groupcount"

hdfs dfs -rm -r -skipTrash ${OUTPUT1} > /dev/null

hdfs dfs -get /datasets/stop_words_en.txt .

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.jab.name="Streaming wordCount" \
    -D mapreduce.job.reduces=8 \
    -files mapper1.py,reducer1.py,stop_words_en.txt \
    -mapper "python mapper1.py" \
    -reducer "python reducer1.py" \
    -input ${INPUT} \
    -output ${OUTPUT1} > /dev/null

hdfs dfs -rm -r -skipTrash ${OUTPUT2} > /dev/null

yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -D mapred.jab.name="Streaming wordCount" \
    -D mapreduce.job.reduces=1 \
    -files mapper2.py,reducer2.py \
    -mapper "python mapper2.py" \
    -reducer "python reducer2.py" \
    -input ${OUTPUT1} \
    -output ${OUTPUT2} > /dev/null

hdfs dfs -cat ${OUTPUT2}/part-00000 | grep 'english,helsing,hesling,shengli,shingle'
```
